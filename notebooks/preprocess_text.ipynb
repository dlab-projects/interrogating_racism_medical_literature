{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94026dd9-18ed-4d5a-9d44-30e42938f421",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Last updated: 08162023  \n",
    "By: Lauren Liao, Pratik Sachdeva  \n",
    "Purpose: data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e1abcb-bda4-46e4-aaae-4d16bdecd7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import wordninja\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS, Phraser\n",
    "from nltk.corpus import words\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ade08-e4f7-4ac0-8ce1-32174f0413eb",
   "metadata": {},
   "source": [
    "**Constants and functions**\n",
    "\n",
    "VALID_WORDS: words that are in the nltk corpus  \n",
    "VALID_WORDS2: words in the wordnet nltk corpus  \n",
    "\n",
    "_americanize_ converts English spellings to American English  \n",
    "_proprocess_ removes punctuations and other miscellaneous steps to make the text more uniform  \n",
    "_clean_underline_words_ cleans the underline words including any misspellings  \n",
    "_token_preprocess_ prepocess the words more individually as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49f5734-5f79-4529-aca8-9df2273c7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID_WORDS is specified separately as VALID_WORDS2 has more words\n",
    "\n",
    "VALID_WORDS = words.words()\n",
    "VALID_WORDS.append('bame')\n",
    "VALID_WORDS.append('coronavirus')\n",
    "VALID_WORDS.append('disadvantaged')\n",
    "VALID_WORDS = [word.lower() for word in VALID_WORDS if len(word) > 1]\n",
    "VALID_WORDS2 = list(wordnet.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6598472-6bb5-4fe1-ae7c-835fbd4127ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert British English spellings to American English\n",
    "def americanize(string):\n",
    "    url = \"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/british_spellings.json\"\n",
    "    british_to_american_dict = requests.get(url).json()    \n",
    "\n",
    "    for british_spelling, american_spelling in british_to_american_dict.items():\n",
    "        string = string.replace(british_spelling, american_spelling)\n",
    "  \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0338f6-1bb9-4625-8449-a8a124377d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses text from pd.series\"\"\"\n",
    "    \n",
    "    # word break\n",
    "    word_break_pattern = r\"\\b(\\w+)\\-?\\n?(\\w+)\\b\"\n",
    "    text = text.apply(lambda x: re.sub(word_break_pattern, r\"\\1\\2\", x, flags=re.MULTILINE))\n",
    "    \n",
    "    # DOI \n",
    "    doi_pattern = r'\\b10\\.\\d{4,9}/[-.;()/:\\w]+'\n",
    "    doi_repl = ''\n",
    "    text = text.str.replace(doi_pattern, doi_repl, regex=True)\n",
    "\n",
    "    # URL \n",
    "    url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "    url_repl = ''\n",
    "    text = text.str.replace(url_pattern, url_repl, regex=True)\n",
    "\n",
    "    # digit\n",
    "    # remove them\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ''\n",
    "    text = text.str.replace(digit_pattern, digit_repl, regex=True)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.str.lower()\n",
    "    # Apostrophes\n",
    "    text = text.str.replace('[’+]', '', regex=True)\n",
    "    # Hyphens\n",
    "    text = text.str.replace('[-]', '', regex=True)\n",
    "    # Replace punctuation with space\n",
    "    text = text.str.replace('[^\\w\\s]', ' ', regex=True)\n",
    "    # Remove new lines\n",
    "    text = text.str.replace('\\n', ' ', regex=True)\n",
    "    \n",
    "    text = text.apply(lambda x: americanize(x))\n",
    "    text = text.str.replace('diskrimination', 'discrimination') # Fixing an typo in the json file\n",
    "    text = text.str.replace('diskourse', 'discourse') # Fixing an typo in the json file\n",
    "    text = text.str.replace('transmision', 'transmission') # Fixing an typo in the json file\n",
    "    text = text.str.replace(' dis ', '')\n",
    "    \n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = text.str.replace(blankspace_pattern, blankspace_repl, regex=True)\n",
    "    text = text.str.strip()\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25fd7994-d992-48f9-9c96-9d93d15b9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\"eff_ect\": \"effect\", \"diff_e_rent\": \"different\", \"le_likely\": \"likely\",\n",
    "                \"black_m\": \"black\", \"th_century\": \"century\", \"disk_ri_minatory\": \"discriminatory\",\n",
    "                \"c_ovid\": \"covid\", \"syn_demi_c\": \"syndemic\", \"fi_rst\": \"first\", \"bene_fi\": \"benefit\",\n",
    "                \"adopt_ee\": \"adoptee\", \"diff_e_rence\": \"difference\", \"disk_ip_line\": \"discipline\",\n",
    "                \"disk_u_sion\": \"discussion\", \"disk_u_sed\": \"discussed\", \"ff_ect\": \"effect\",\n",
    "                \"eff_t\": \"effect\", \"disk_lo_sure\": \"disclosure\",\n",
    "                \"diff_cult\": \"difficult\", \"fi_n_ding\": \"finding\",\n",
    "                \"fi_nanci_al\": \"financial\",\n",
    "                \"eff_ect_ive\": \"effective\",\n",
    "                \"sars_cov\": \"sars_cov2\",\n",
    "                \"detain_ee\": \"detainee\",\n",
    "                \"multi_sector_al\": \"multisectoral\",\n",
    "                \"peri_natal\": \"perinatal\",\n",
    "                \"com_bidi_tie\": \"comorbidity\",\n",
    "                \"prior_iz_ation\": \"prioritization\",\n",
    "                \"disk_rim_ate\": \"discriminate\",\n",
    "                \"perspective_www_lancet_com\": \"perspective\",\n",
    "                \"protected_copyright_bmj\": \"protected\",\n",
    "                \"br_exit\": \"brexit\",\n",
    "                \"told_bmj\": \"told\",\n",
    "                \"nh_england\": \"england\",\n",
    "                \"st_century\": \"century\",\n",
    "                \"ff_ec_ted\": \"effected\",\n",
    "                \"sign_cant_ly\": \"significantly\",\n",
    "                \"disk_u_sing\": \"discussing\",\n",
    "                \"scientifi_c\": \"scientific\",\n",
    "                \"diff_cul_tie\": \"difficulty\",\n",
    "                \"fi_nd\": \"find\",\n",
    "                \"appendix_p\": \"appendix\",\n",
    "                \"intersection_ali_ty\": \"intersectionality\",\n",
    "                \"bio_logic\": \"biologic\",\n",
    "                \"diff_er\": \"differ\",\n",
    "                \"fl_u\": \"flu\",\n",
    "                \"su_ff_e_ring\": \"suffering\",\n",
    "                \"def_n_ion\": \"definition\",\n",
    "                \"con_founder\": \"confounder\",\n",
    "                \"ref_l_ect\": \"reflect\",\n",
    "                \"eff_ort\": \"effort\",\n",
    "                \"p_harm_co_logical\": \"pharmacological\",\n",
    "                \"sign_cant\": \"significant\",\n",
    "                \"disk_om_fort\": \"discomfort\",\n",
    "                \"ident_cation\": \"identification\",\n",
    "                \"fi_lm\": \"film\",\n",
    "                \"intersection_al\": \"intersectional\",\n",
    "                \"un_adjusted\": \"unadjusted\",\n",
    "                \"un_met\": \"unmet\",\n",
    "                \"disk_tio_nary\": \"dictionary\",\n",
    "                \"disk_us\": \"discuss\",\n",
    "                \"co_payment\": \"copayment\",\n",
    "                \"epi_de_mio_logi\": \"epidemiology\",\n",
    "                \"co_nfl_ict\": \"conflict\",\n",
    "                \"enroll_l_ment\": \"enrollment\",\n",
    "                \"md_g_target\": \"target\",\n",
    "                \"minor_ed\": \"minored\",\n",
    "                \"stigma_ti_z_ation\": \"stigmatization\",\n",
    "                \"med_line\": \"medline\",\n",
    "                \"ha_emo_dynamic\": \"haemodynamic\",\n",
    "                \"lef_fall\": \"fall\",\n",
    "                \"eff_ect_ive_ness\": \"effectiveness\",\n",
    "                \"sector_al\": \"sectoral\",\n",
    "                \"peri_natal_mortality\": \"perinatal_mortality\",\n",
    "                \"fl_ow\": \"flow\",\n",
    "                \"fulfill_l\": \"fulfill\",\n",
    "                \"wels_ing\": \"welsing\",\n",
    "                \"disk_har_ged\": \"discharged\",\n",
    "                \"disk_lose\": \"disclose\",\n",
    "                \"eff_ect_iv_ely\": \"effectively\",\n",
    "                \"fi_nanci\": \"financial\",\n",
    "                \"appall_ling\": \"appalling\",\n",
    "                \"car_dio\": \"cardio\",\n",
    "                \"lgbt_q\": \"lgbtq\",\n",
    "                \"km_iet_owicz\": \"kmietowicz\",\n",
    "                \"gestation_al\": \"gestational\",\n",
    "                \"exclusion_ary\": \"exclusionary\",\n",
    "                \"uni_variate\": \"univariate\",\n",
    "                \"mr_chau_dha_ry\": \"mr_chaudhary\",\n",
    "                \"lund_berg\": \"lundberg\",\n",
    "                \"mc_score\": \"score\",\n",
    "                \"co_variate\": \"covariate\",\n",
    "                \"bidi_l\": \"bidil\",\n",
    "                \"disk_rete\": \"discrete\",\n",
    "                \"cha_nag_paul\": \"chanagpaul\",\n",
    "                \"plunk_et\": \"plunket\",\n",
    "                \"ff_ec_ting\": \"effecting\",\n",
    "                \"atri_cul\": \"articul\",\n",
    "                \"ap_ea\": \"apea\",\n",
    "                \"g_astro\": \"gastro\",\n",
    "                \"giridhara_da\": \"giridharadas\",\n",
    "                \"cadaver_ic\": \"cadaveric\",\n",
    "                \"pro_actively\": \"proactively\",\n",
    "                \"cor_tico_steroid\": \"corticosteroid\",\n",
    "                \"comp_licit\": \"complicit\",\n",
    "                \"du_ally\": \"dually\",\n",
    "                \"enroll_l\": \"enroll\",\n",
    "                \"dia_logic\": \"dialogic\"\n",
    "               }\n",
    "removals = [\"et_al\", \"md_g\", \n",
    "            \"n_engl_j_med\", \"medicine_downloaded_ne_j\",\n",
    "            \"spec_c\", \"disk_us_ion\", \"copyright_massachusetts_medical\", \"association_right_reserved\",\n",
    "            \"h_ic\", \"nh_trust\", \"ne_j_org\", \"sd_g\", \"org_depaul_university_march\",\n",
    "            \"lm_ic\", \"def_ned\", \"iz_ation\", \"e_g\", \"downloaded_depaul_university_pre\",\n",
    "            \"ch_w\", \"ident_ed\",  \"n_cd\", \"disk_ed\", \"fi_el\", \"en_ce\", \"bmj_com\",\n",
    "            \"e_gfr\", \"rm_n_cah\", \"fl_exner\", \"ori_non_pac_c\", \"blu_men_bach\", \"used_uk_cat\",\n",
    "            \"ey_w\", \"bmj_bmj_doi\", \"suf_fi\", \"na_em\", \"fi_cia_l\", \"md_mph\",\n",
    "            \"e_cozy_stem\", \"ischemic_heart\", \"cite_bmj\", \"fi_gu\", \"u_without_permission_copyright_massachusetts\",\n",
    "            \"def_ne\", \"cie_nt\", \"p_h\", \"pac_c\", \"b_awa_garb\", \"ori_non\", \"de_bow_ale\", \"hall_ig\", \"p_harm\",\n",
    "            \"full_story_doi\", \"w_re\", \"appendix_pp\", \"ne_j_org_november\", \"p_harm_co\",\n",
    "            \"fi_ed\",\n",
    "            \"h_arge\",\n",
    "            \"comment_www_lancet_com\",\n",
    "            \"ac_uk\",\n",
    "            \"disk_rep\",\n",
    "            \"disk_rim_ted\", \"jama_network_open\", \"uh_c\", \n",
    "            \"see_supplementary\", \"use_u_without_permission\", \"rc_gp\", \"ka_er\", \n",
    "            \"ne_j_org_september\",\n",
    "            \"reprinted_jama\",\n",
    "            \"edu_opinion_jama_com\",\n",
    "            \"zur_cruz\",\n",
    "            \"disk_y\",\n",
    "            \"subscribe_l_doi_published\",\n",
    "            \"bmj_doi\",\n",
    "            \"eff_cie\",\n",
    "            \"disk_ip\",\n",
    "            \"colo_gist\",\n",
    "            \"doi_pmid\",\n",
    "            \"rr_ci\",\n",
    "            \"caput_tion\",\n",
    "            \"lo_cum\",\n",
    "            \"cb_pr\",\n",
    "            \"ph_o\",\n",
    "            \"spec_call\",\n",
    "            \"disk_rim\",\n",
    "            \"te_ques\",\n",
    "            \"www_lancet_com_vol\",\n",
    "            \"disk_nec\",\n",
    "            \"ne_phr\",\n",
    "            \"king_fund\",\n",
    "            \"http_www\",\n",
    "            \"ali_ty\",\n",
    "            \"jama_com_reprinted_jama\",\n",
    "            \"la_cell\",\n",
    "            \"su_ff\",\n",
    "            \"fi_ce\",\n",
    "            \"cpp_r\",\n",
    "            \"aldo_ster\",\n",
    "            \"ey_w_copyright\",\n",
    "            \"ff_able\",\n",
    "            \"ful_fi\",\n",
    "            \"thro_po\",\n",
    "            \"gl_omer\",\n",
    "            \"rim_mer\",\n",
    "            \"rd_ci\",\n",
    "            \"logi_st\",\n",
    "            \"pre_ey\",\n",
    "            \"com_reprinted_jama\",\n",
    "            \"cia_l\",\n",
    "            \"pac_c_people\",\n",
    "            \"chau_dha\",\n",
    "            \"org_june\",\n",
    "            \"copyright_bmj_first\",\n",
    "            \"ref_l\",\n",
    "            \"ne_j_org_july\",\n",
    "            \"march_personal\",\n",
    "            \"lancet_com\",\n",
    "            \"e_ring\",\n",
    "            \"co_nfl\",\n",
    "            \"demi_c\",\n",
    "            \"iv_ely\",\n",
    "            \"diff_e\",\n",
    "            \"cd_cell\",\n",
    "            \"ec_ted\",\n",
    "            \"co_bucci\",\n",
    "            \"disk_har\",\n",
    "            \"disk_ipl\",\n",
    "            \"ary_team\",\n",
    "            \"lynch_et_al\"\n",
    "           ]\n",
    "\n",
    "def clean_underline_words(sentences, replacements=replacements, removals=removals):\n",
    "    clean_sentences = [[replacements.get(item, item) for item in sentence] for sentence in sentences]\n",
    "    return [[item for item in clean_sentence if item not in removals] for clean_sentence in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01563a0-e5a9-4d41-a627-099a7d22abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_preprocess(text, return_excluded=False):\n",
    "    \"\"\"Preprocesses text from pd.series\"\"\"\n",
    "    sentences = text.to_list()\n",
    "    sentences = [nltk.word_tokenize(s) for s in sentences]\n",
    "    stop = stopwords.words('english')\n",
    "    stop.append('also')\n",
    "    stop.append('eg')\n",
    "    stop.append('aliz')\n",
    "    stop.append('mi')\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # following: [[float(y) for y in x] for x in l] syntax\n",
    "    sentences = [[lemmatizer.lemmatize(token) for token in s if token not in stop] for s in sentences]\n",
    "    \n",
    "    ## using word ninja\n",
    "    ## added bame into the special words to not split by wordninja\n",
    "    split_sentences = [[wordninja.split(token) if (token != \"covid\" and token != \"bame\") else [token] for token in s] for s in sentences]\n",
    "    # flatten this \n",
    "    split_sentences = [[token for s in sublist for token in s] for sublist in split_sentences]\n",
    "\n",
    "    # do this again for split_sentences\n",
    "    split_sentences = [[lemmatizer.lemmatize(token) for token in s if token not in stop] for s in split_sentences]\n",
    "    \n",
    "    bigram = Phrases(split_sentences, min_count=10, threshold=100, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "    trigram = Phrases(bigram[split_sentences], min_count=10, threshold=50, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "    \n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    trigram_phraser = Phraser(trigram)\n",
    "    processed_sentences = [trigram_phraser[bigram_phraser[s]] for s in split_sentences]\n",
    "    \n",
    "    clean_underline_sentences = clean_underline_words(processed_sentences)\n",
    "        \n",
    "    # add to check what is valid\n",
    "    processed_sentences_new = [[word \n",
    "                if (word in VALID_WORDS) | (\"_\" in word) \n",
    "                else word\n",
    "                if (word in [\"psa\", \"usa\", \"adhd\", \"usda\", \"asia\", \"paid\", \"tzar\"]) | (len(word) > 4 and word in VALID_WORDS2)  \n",
    "                else \"\" for word in s] for s in tqdm(clean_underline_sentences)]\n",
    "\n",
    "    processed_sentences_new = [[word for word in s if len(word) > 0] for s in tqdm(processed_sentences_new)]\n",
    "    \n",
    "\n",
    "    if(return_excluded):\n",
    "        # take the difference for pairwise comparison for each article\n",
    "        def get_unique_excluded(list1, list2):\n",
    "            unique_words = set()\n",
    "\n",
    "            for sublist1, sublist2 in zip(list1, list2):\n",
    "                set1 = set(sublist1)\n",
    "                set2 = set(sublist2)\n",
    "                unique_words.update(set1 - set2)\n",
    "\n",
    "            return list(unique_words)\n",
    "        excluded_words = get_unique_excluded(clean_underline_sentences, processed_sentences_new)\n",
    "        return(processed_sentences_new, excluded_words)\n",
    "    \n",
    "    return(processed_sentences_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c83e0-7cab-4684-85a5-aaffbb06181e",
   "metadata": {},
   "source": [
    "**Data loading**  \n",
    "reading data requires openpyxl download separately as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c901290-e3f8-43c5-8dcc-08dfb393615f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '../data/full_data.xlsx'\n",
    "data = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53684448-72c0-4385-8323-0e297af5f66c",
   "metadata": {},
   "source": [
    "## Data preprocessing step 1: uniform text cleaning\n",
    "data preprocessing steps as outlined here:  \n",
    "(1) merging multiple texts into a single column  \n",
    "(2) find and replace word break patterns residue from pdf  \n",
    "(3) replace doi, url, digit with respective pattern recognizers  \n",
    "(4) standardize to lower case then removing apostrophes, hypens, punctuations, new lines  \n",
    "(5) standardize to American English  \n",
    "(6) correct misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ace3e8-d993-44e2-916a-eef6ebb0fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab text columns\n",
    "text_cols = data.filter(regex='^full_text')\n",
    "# Merge text columns into a single column\n",
    "data['full_text'] = text_cols.apply(\n",
    "    lambda x: ' '.join(x.dropna()),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a0881a6-0536-450b-8823-5c920b961f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 1.64 s, total: 1min 37s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['text'] = preprocess(data['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d1581-ac0d-411d-b84c-7e6fd5a68322",
   "metadata": {},
   "source": [
    "## Data preprocessing step 2: Tokenizing and preprocess tokenized text\n",
    "create sentences and tokenize each word to vectorize the words to see what is most similar  \n",
    "\"sentences\" here refers to the full text for the separate texts  \n",
    "processing (tokenizing) is done to  \n",
    "(1) remove stop words and words shouldn't be split  \n",
    "(2) lemmatize the tokens   \n",
    "(3) split the connected words using wordninja   \n",
    "(4) merge bigrams or trigrams  \n",
    "(5) check the word if it is valid from VALID_WORD or has underscore otherwise check if the word is in a specific list or (it is more than 4 letter and in the VALID_WORD2 corpus)  \n",
    "(6) clean the underline words and fix misspellings\n",
    "(7) return excluded words if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8290d343-4ab2-47bc-9a09-47476f89086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 870/870 [16:40<00:00,  1.15s/it]\n",
      "100%|███████████████████████████████████████| 870/870 [00:00<00:00, 7824.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 10s, sys: 3.95 s, total: 17min 14s\n",
      "Wall time: 17min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['processed_text'], excluded_words = token_preprocess(data['text'], return_excluded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c9eff52-5cee-4f10-adaa-cb97cb0c5f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.to_pickle('../data/full_data_w_processedtext.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910dfa5-b109-492b-9700-14f5837bf401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textanalysis",
   "language": "python",
   "name": "textanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
